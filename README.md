# llModel Telegram Bot

## Overview

llModel is a Telegram bot designed to provide a unified interface for interacting with various Large Language Models (LLMs). A key feature of this bot is its ability to stream responses in real-time, mimicking the experience of native LLM applications. It also visually distinguishes and handles "reasoning" or "thought" blocks from the LLMs, displaying them as quotes which become collapsible once the full thought process is delivered.

The project consists of two main components:
1.  **The Telegram Bot (`ai_bot`)**: Handles user interactions, message formatting, state management, and communication with the LLM proxy.
2.  **The LLM Proxy Server (`llm_proxy`)**: A Flask-based server that receives requests from the Telegram bot, routes them to the appropriate LLM provider (via LiteLLM), and streams responses back.

## Features

*   **Multi-LLM Support**: Interact with different LLMs through a single bot interface.
*   **Real-time Streaming**: Responses are streamed to the user as they are generated by the LLM.
*   **Reasoning Block Handling**: "Think" or "reasoning" blocks from LLMs are displayed as quotes. Completed reasoning blocks become collapsible to save space.
*   **Context Management**: Supports conversation history to provide context to LLMs.
*   **User Profiles & Settings**: Users can manage basic profile information and select preferred models.
*   **Referral System**: Basic referral tracking.

## Project Structure

```
.
â”œâ”€â”€ llm_proxy/        # Flask server for LLM routing and streaming
â”‚   â”œâ”€â”€ main.py       # Main Flask application
â”‚   â”œâ”€â”€ utils.py      # Utility functions for LLM interaction
â”‚   â”œâ”€â”€ load_data.py  # Loads provider configurations (e.g., from providers.yaml)
â”‚   â””â”€â”€ providers.yaml # Example (you'''ll need to create/configure this)
â”œâ”€â”€ routers/          # Aiogram routers for handling different bot commands/messages
â”‚   â””â”€â”€ router.py     # Main message handling logic
â”œâ”€â”€ models/           # Database models (e.g., User)
â”œâ”€â”€ services/         # Business logic services (e.g., message service)
â”œâ”€â”€ keyboards/        # Aiogram keyboard layouts
â”œâ”€â”€ middleware/       # Aiogram middlewares
â”œâ”€â”€ main.py           # Main entry point for the Telegram bot
â”œâ”€â”€ strings.py        # String constants and templates
â”œâ”€â”€ settings.py       # Bot settings
â”œâ”€â”€ requirements.txt  # Python dependencies
â”œâ”€â”€ .env              # Environment variables for the bot (to be created)
â””â”€â”€ README.md         # This file
```

## Setup and Installation

### Prerequisites

*   Python 3.10+
*   Access to various LLM APIs and their respective API keys.

### 1. Clone the Repository

```bash
git clone <repository-url>
cd <repository-directory>
```

### 2. Install Dependencies

It'''s recommended to use a virtual environment:

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### 3. Configure Environment Variables

You'''ll need to create two `.env` files: one in the project root for the Telegram bot, and one inside the `llm_proxy` directory for the proxy server.

**a) For the Telegram Bot (create `.env` in the project root):**

```env
# .env
BOT_TOKEN="YOUR_TELEGRAM_BOT_TOKEN"
AI_STREAM_URL="http://127.0.0.1:5050/stream/" # URL of your llm_proxy server
# AI_API_KEY="YOUR_DEFAULT_PROXY_API_KEY" # Optional: if your proxy requires a key for access
```
*   `BOT_TOKEN`: Your Telegram Bot Token obtained from BotFather.
*   `AI_STREAM_URL`: The URL where your `llm_proxy` server will be running.

**b) For the LLM Proxy Server (create `.env` inside the `llm_proxy` directory):**

The `llm_proxy` uses a `providers.yaml` file (you'''ll need to create this in the `llm_proxy` directory) to define how to access different LLM providers. The API keys themselves are loaded from environment variables specified within `providers.yaml`.

**Example `llm_proxy/providers.yaml`:**
```yaml
# llm_proxy/providers.yaml
- model_name: openai_models # A group name for your OpenAI models
  litellm_params:
    model: gpt-4o # Default model in this group, can be overridden
    api_key_env: OPENAI_API_KEYS_JSON # Env variable holding JSON list of keys
  # Add other litellm_params as needed
- model_name: anthropic_models
  litellm_params:
    model: claude-3-opus-20240229
    api_key_env: ANTHROPIC_API_KEYS_JSON
# ... other providers
```

**Example `llm_proxy/.env`:**
```env
# llm_proxy/.env
OPENAI_API_KEYS_JSON='["sk-yourOpenAIKey1", "sk-yourOpenAIKey2"]'
ANTHROPIC_API_KEYS_JSON='["sk-ant-yourAnthropicKey1"]'
# GROQ_API_KEYS_JSON='["gsk_yourGroqKey1"]'
# Add other API key environment variables as defined in your providers.yaml
```
*   The keys in this `.env` file (e.g., `OPENAI_API_KEYS_JSON`) must match the `api_key_env` values in your `llm_proxy/providers.yaml`.
*   The values are JSON strings representing a list of API keys for that provider.

### 4. Initialize Database

The bot uses SQLite. The database file (`db.sqlite3` by default, configurable in `settings.py`) will be created automatically when the bot first runs if it doesn'''t exist.

## How to Run

**1. Start the LLM Proxy Server:**

Navigate to the `llm_proxy` directory and run the Flask server:
```bash
cd llm_proxy
python main.py
```
By default, it should run on `http://127.0.0.1:5050`.

**2. Start the Telegram Bot:**

In a new terminal, navigate to the project root directory and run the bot:
```bash
cd .. # if you are still in llm_proxy directory
python main.py
```

Your bot should now be running and connected to your LLM proxy.

## Usage

Interact with your bot on Telegram:
*   Send any text message to start a conversation with the default LLM.
*   Use commands like `/profile_image` to set a profile picture.
*   Explore menu options for changing models, viewing profile, etc. (triggered by specific text like "ðŸ‘¤ ÐŸÑ€Ð¾Ñ„Ð¸Ð»ÑŒ").

## Project Links

*   **Presentation**: [Google Slides Presentation](https://docs.google.com/presentation/d/12APUGdPgjhjVe6kH3aq6R2dEfocMhpoX/edit?usp=sharing&ouid=109181077945453147878&rtpof=true&sd=true1)
*   **Public Bot Instance**: [t.me/llModel_bot](https://t.me/llModel_bot) (Note: This instance may be running a specific version and configuration).

## Contributing

Contributions are welcome! Please feel free to submit pull requests or open issues for bugs, feature requests, or improvements.
